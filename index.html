<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tianyi Wu (Ethan) - Personal Academic Page</title>
    <style>
        :root {
            --primary-color: #005A9C; /* Prussian Blue */
            --secondary-color: #333333; /* Dark Gray for Nav */
            --accent-color-blue: #00BFFF; /* Deep Sky Blue */
            --accent-color-green: #4DB870; /* Accent Green */
            --light-bg-color: #F5F5F5;
            --text-color: #333333;
            --card-bg-color: #FFFFFF;
            --card-border-color: #E0E0E0;
            --heading-font: 'Montserrat', sans-serif;
            --body-font: 'Open Sans', sans-serif;
        }

        body {
            font-family: var(--body-font);
            line-height: 1.7;
            margin: 0;
            padding: 0;
            background-color: var(--light-bg-color);
            color: var(--text-color);
            padding-top: 60px; /* Adjusted for slightly taller nav */
        }
        header {
            background-color: var(--primary-color);
            color: #FFFFFF;
            padding: 2.5rem 1.5rem; /* Increased padding */
            text-align: center;
        }
        header h1 {
            margin-bottom: 0.5rem;
            color: #FFFFFF;
            font-family: var(--heading-font);
            font-size: 2.5em; /* Larger main title */
        }
        header p {
            font-size: 1.15em;
            margin-bottom: 0.5rem;
            opacity: 0.9;
        }
        nav {
            background-color: var(--secondary-color);
            padding: 1rem 0; /* Increased padding */
            text-align: center;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            z-index: 1000;
            box-shadow: 0 2px 8px rgba(0,0,0,0.25);
        }
        nav a {
            color: #FFFFFF;
            margin: 0 20px;
            text-decoration: none;
            font-weight: bold;
            font-size: 1.05em; /* Slightly larger nav links */
            transition: color 0.3s ease;
        }
        nav a:hover, nav a.active {
            color: var(--accent-color-blue);
        }
        .container {
            width: 85%;
            max-width: 1100px; /* Slightly reduced for tighter content flow */
            margin: 30px auto;
            padding: 30px;
            background-color: var(--card-bg-color);
            box-shadow: 0 4px 20px rgba(0,0,0,0.1);
            border-radius: 10px;
        }
        h1, h2, h3 {
            color: var(--primary-color);
            font-family: var(--heading-font);
        }
        h2 {
            font-size: 1.8em; /* Adjusted size */
            border-bottom: 2px solid var(--primary-color);
            padding-bottom: 12px;
            margin-top: 2em;
            margin-bottom: 1.5em;
        }
        h3 {
            font-size: 1.4em; /* Adjusted size */
            margin-top: 1.5em;
            margin-bottom: 1em;
            color: #004170; /* Darker shade for H3 */
        }
        .section {
            margin-bottom: 35px;
            padding-bottom: 30px;
            border-bottom: 1px solid #EAEAEA; /* Slightly lighter border */
        }
        .section:last-child {
            border-bottom: none;
            padding-bottom: 10px; /* Less padding for the last section */
        }
        .profile-img {
            width: 160px;
            height: 160px;
            border-radius: 50%;
            display: block;
            margin: 30px auto;
            border: 4px solid var(--primary-color);
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }
        .publication, .project {
            margin-bottom: 25px;
            padding: 20px;
            border: 1px solid var(--card-border-color);
            border-radius: 8px;
            background-color: #FAFAFA; /* Slightly off-white card background */
            transition: box-shadow 0.3s ease, transform 0.3s ease;
        }
        .publication:hover, .project:hover {
            box-shadow: 0 6px 18px rgba(0, 75, 128, 0.18);
            transform: translateY(-4px);
        }
        .publication .title, .project .title { /* Specific class for title */
            font-size: 1.25em;
            font-weight: bold;
            color: var(--primary-color);
            display: block;
            margin-bottom: 8px;
        }
        .publication .authors, .project .role {
            font-style: italic;
            color: #555555;
            display: block;
            margin-bottom: 8px;
            font-size: 0.95em;
        }
        .publication .venue, .project .date {
            color: #777777;
            font-size: 0.9em;
            margin-bottom: 10px;
            display: block;
        }
        .publication .abstract, .project .description {
            font-size: 0.95em;
            line-height: 1.6;
            margin-bottom: 12px;
        }
        .publication .links a, .project .links a { /* For links within cards */
            text-decoration: none;
            color: var(--accent-color-blue);
            font-weight: bold;
            font-size: 0.9em;
            margin-right: 15px;
        }
        .publication .links a:hover, .project .links a:hover {
            text-decoration: underline;
            color: var(--primary-color);
        }
        .external-link-icon { /* Basic icon style */
            display: inline-block;
            width: 12px;
            height: 12px;
            margin-left: 4px;
            background-image: url('data:image/svg+xml;charset=US-ASCII,%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%2024%2024%22%20fill%3D%22%23007bff%22%3E%3Cpath%20d%3D%22M19%2019H5V5h7V3H5c-1.11%200-2%20.9-2%202v14c0%201.1.9%202%202%202h14c1.1%200%202-.9%202-2v-7h-2v7zM14%203v2h3.59l-9.83%209.83%201.41%201.41L19%206.41V10h2V3h-7z%22%2F%3E%3C%2Fsvg%3E');
            background-repeat: no-repeat;
            background-size: contain;
            vertical-align: middle;
        }
        .tag {
            display: inline-block;
            background-color: var(--accent-color-blue);
            color: white;
            padding: 5px 12px;
            margin: 4px;
            border-radius: 5px;
            font-size: 0.88em;
            font-weight: bold;
        }
        .cv-link a {
            display: inline-block;
            background-color: var(--accent-color-green);
            color: white;
            padding: 12px 28px;
            text-decoration: none;
            border-radius: 6px;
            font-weight: bold;
            font-size: 1.05em; /* Larger CV button text */
            transition: background-color 0.3s ease, transform 0.3s ease;
            margin-top: 10px; /* Space above CV button */
        }
        .cv-link a:hover {
            background-color: #3a8f5a;
            transform: translateY(-2px);
        }
        footer {
            text-align: center;
            padding: 30px;
            background-color: #222222; /* Even darker footer */
            color: #CCCCCC;
            margin-top: 40px;
            font-size: 0.9em;
        }
        footer p {
            margin: 6px 0;
        }
        ul {
            list-style-position: outside;
            padding-left: 22px;
        }
        li {
            margin-bottom: 10px;
        }
        /* Helper class for small text, e.g., role/date lines */
        .small-text {
            font-size: 0.9em;
            color: #666;
            margin-bottom: 5px;
        }
    </style>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@600;700&family=Open+Sans:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>

    <header>
        <h1>Tianyi Wu (Ethan)</h1>
        <p>Undergraduate Student, Information and Computing Science, Xi'an Jiaotong-Liverpool University (XJTLU)</p>
        <p>Focusing on Artificial Intelligence, Multimodal Emotion Recognition, and their applications in Education and Biomedicine.</p>
    </header>

    <nav>
        <a href="#home">Home</a>
        <a href="#about">About Me</a>
        <a href="#research">Research</a>
        <a href="#projects">Projects</a>
        <a href="#skills">Skills</a>
        <a href="#cv">CV</a>
        <a href="#contact">Contact</a>
    </nav>

    <div class="container">

        <section id="home" class="section">
            <h2>Welcome</h2>
            <img src="45x45-吴天一.jpg" alt="Tianyi Wu - Professional Photo" class="profile-img">
            <p>Tianyi Wu, an undergraduate student in Information and Computing Science at Xi'an Jiaotong-Liverpool University, specializing in Artificial Intelligence, Multimodal Emotion Recognition, and its applications in Education and Biomedicine.</p>
            <h3>Key Highlights</h3>
            <ul>
                <li><a href="#research-M3RNet">M3RNet: Enhancing Multimodal Emotion Recognition (ICCV 2025 Submission)</a></li>
                <li><a href="#research-EmoMA-Net">EmoMA-Net: Emotion Recognition in Adaptive Educational Systems (ICBDE 2024 Published)</a></li>
            </ul>
            <h3>Research Interests</h3>
            <p>
                <span class="tag">Multimodal Learning</span>
                <span class="tag">Emotion Computation</span>
                <span class="tag">Deep Learning</span>
                <span class="tag">Computer Vision</span>
                <span class="tag">Adaptive Systems</span>
                <span class="tag">Biomedical Engineering</span>
                <span class="tag">Machine Learning</span>
            </p>
        </section>

        <section id="about" class="section">
            <h2>About Me</h2>
            <h3>Educational Background</h3>
            <p><strong class="title">Xi'an Jiaotong-Liverpool University (XJTLU), Suzhou, China</strong>
                Bachelor's in Information and Computing Science (Sept. 2022 - Jul. 2026 Expected)<br>
                GPA: 4.0/4.0<br>
                Honors: 2023-2024 Xi'an Jiaotong-Liverpool University Undergraduate Academic Scholarship (Top 10%)
            </p>
            <p><strong>Online Courses & Self-Learning:</strong></p>
            <ul>
                <li>UC Los Angeles, Online: Electrical Engineering - IoT and UAV Array Network Communication System Design (Nov. 2024)</li>
                <li>Imperial College London, Online: Machine Learning in Biomedical Sciences and Bioengineering (Mar. 2024)</li>
                <li>UC Berkeley, Online (Self-learning): CS61B: Data Structures and Algorithms (Apr. 2023)</li>
            </ul>

            <h3>Research Interests and Passion</h3>
            <p>My research focuses on advancing multimodal emotion recognition through innovative deep learning architectures. My work on M3RNet involves integrating memory-augmented transformers to improve long-term feature retention for more accurate emotional cue detection. EmoMA-Net aims to enhance adaptive learning environments by providing real-time feedback on student stress levels using physiological data. I am passionate about applying these technologies to create impactful solutions in education and biomedical fields.</p>

            <h3>Career Goals and Vision</h3>
            <p>I aim to pursue further academic studies and contribute to cutting-edge research in artificial intelligence and human-computer interaction, with a vision to develop technologies that can meaningfully improve people's lives.</p>

            <h3>Personal Interests and Activities</h3>
            <p>Piano (Level 10 certification, Shanghai Conservatory of Music), Street Dance (Level 4 certification, China Street Dance Association), Badminton, Movies. <br>
            Member, Xi'an Jiaotong-Liverpool University Psychology Association (PSYA).</p>
        </section>

        <section id="research" class="section">
            <h2>Research / Publications</h2>
            
            <h3>Awaiting Publication Results</h3>
            <div class="publication" id="research-M3RNet">
                <span class="title">M3RNet: Enhancing Multimodal Emotion Recognition with Memory-Augmented Transformers on Physiological, Behavioral, and Video Modalities</span>
                <span class="authors">Wu, T., Huang, Y., & Purwanto, E.</span>
                <span class="venue">Submitted to: International Conference on Computer Vision (ICCV 2025)</span>
                <p class="small-text">Role: First Author</p>
                <p class="abstract">M3RNet advances multimodal emotion recognition by integrating a memory-augmented transformer, the Memory Stream, to improve long-term feature retention. This system, utilizing the TMBL framework, enables sophisticated multimodal fusion that boosts accuracy across datasets like PPB-Emo and CL-Drive. Introduced M3RNet, featuring the Memory Stream, to enhance the transformer's ability to maintain long-term feature memory. Enhanced accuracy and stability across diverse emotional datasets. Implemented TMBL to fuse physiological and video data, boosting effectiveness in managing cross-modal interactions. Achieved over 9.15% improvement in accuracy on the PPB-Emo dataset and surpassed baseline models by 10.09% on the CL-Drive dataset.</p>
            </div>
            <div class="publication">
                <span class="title">Phy-FusionNet: A Memory-Augmented Transformer for Multimodal Emotion Recognition with Periodicity and Contextual Attention</span>
                <span class="authors">Purwanto, E., Wu, T., & Huang, Y.</span>
                <span class="venue">Submitted to: IEEE Transactions on Affective Computing</span>
            </div>

            <h3>Published</h3>
            <div class="publication" id="research-EmoMA-Net">
                <span class="title">EmoMA-Net: A Novel Model for Emotion Recognition Using Hybrid Multimodal Neural Networks in Adaptive Educational Systems</span>
                <span class="authors">Wu, T., Huang, Y., Craig, P., & Purwanto, E. (2024)</span>
                <span class="venue">In Proceedings of The 7th International Conference on Big Data and Education (ICBDE 2024), Paper No. CD1068, September 24-26, 2024, Trinity College, University of Oxford, UK.</span>
                <p class="small-text">Role: First Author</p>
                <p class="abstract">Developed EmoMA-Net, a novel multimodal neural network for real-time emotion recognition in educational settings. Utilizes physiological data to enhance adaptive learning by providing feedback on student stress levels. Designed framework integrating LSTM and CNNs. Incorporated Convolutional Block Attention Module (CBAM) for optimized feature extraction. Utilized LSTM for enhanced temporal analysis. Achieved up to 99.66% prediction accuracy on the WESAD dataset.</p>
                <p class="links">
                    <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=o00kcT4AAAAJ&citation_for_view=o00kcT4AAAAJ:u5HHmVD_uO8C" target="_blank">View on Google Scholar <span class="external-link-icon"></span></a>
                </p>
            </div>
            <div class="publication">
                <span class="title">Enhancing Multilingual Emotion Classification with Attention Mechanism for Transnational Education</span>
                <span class="authors">Wu, T., Huang, Y., Purwanto, E., Juwono, F., & Tang, F. (2025)</span>
                <span class="venue">In Proceedings of the 2025 International Conference on Artificial Intelligence and Education (ICAIE 2025), May 14-16, 2025, Suzhou, China.</span>
            </div>
        </section>

        <section id="projects" class="section">
            <h2>Project Experience</h2>
            <div class="project">
                <span class="title">Daily Reading Tracker Design</span>
                <span class="date">Timeline: Expected Apr. 2025</span>
                <p class="role">Role: Developer</p>
                <p class="description">Developed a secure and efficient reading tracker system with user authentication (registration, email verification, password encryption, JWT session management), role assignment, and password recovery. Enhanced reading log management with multi-criteria search. Built front end with HTML5, CSS3, JavaScript and back end with Spring Boot for CRUD APIs. Implemented front-end and back-end data validation. Modularized back-end services and used Spring Security. Conducted unit, integration (Mockito, Spring Boot Test), and load tests.</p>
                <p>Technologies: <span class="tag">Java</span> <span class="tag">Spring Boot</span> <span class="tag">HTML5</span> <span class="tag">CSS3</span> <span class="tag">JavaScript</span> <span class="tag">JWT</span> <span class="tag">Mockito</span></p>
            </div>
            <div class="project">
                <span class="title">Kaggle - RSNA 2024 Lumbar Spine Degenerative Classification</span>
                <span class="date">Timeline: Sept. 2024</span>
                <p class="role">Role: Developer</p>
                <p class="description">Developed a multi-stage deep learning model (ResNet-UNet) to detect and classify degenerative lumbar spine conditions from MRI images. Used PCA for feature extraction and dimensionality reduction; image preprocessing (histogram equalization, Gaussian filtering). Integrated ResNet (encoder) and UNet (decoder) with skip connections. Applied multi-task learning for severity prediction. Optimized with weighted binary cross-entropy loss, temperature scaling, and ensemble methods (soft voting).</p>
                <p>Technologies: <span class="tag">Python</span> <span class="tag">Deep Learning</span> <span class="tag">ResNet</span> <span class="tag">UNet</span> <span class="tag">PCA</span></p>
            </div>
            <div class="project">
                <span class="title">Student Data Analysis</span>
                <span class="date">Timeline: Mar. 2024 - Jun. 2024</span>
                <p class="role">Role: Analyst</p>
                <p class="description">Performed preprocessing and PCA on student data. Classified student projects using decision trees, random forests, SVM, and Naive Bayes. Built an ensemble classifier. Fitted feature distributions using GMM, k-means, and hierarchical clustering.</p>
                <p>Technologies: <span class="tag">Python</span> <span class="tag">Scikit-learn</span> <span class="tag">PCA</span> <span class="tag">Classification</span> <span class="tag">Clustering</span></p>
            </div>
             <div class="project">
                <span class="title">Regression Analysis and Deep Learning Applications</span>
                <span class="date">Timeline: Jan. 2024 - May 2024</span>
                <p class="role">Role: Developer</p>
                <p class="description">Implemented regression algorithms and multivariate models. Optimized linear/logistic regression with gradient descent. Utilized CNNs and MLPs for MNIST classification. Developed RNN models (PyTorch) for text classification, optimized with SGD and NLLLoss. Built sequence-to-sequence models and a CNN for image classification on MNIST, improved with data augmentation and regularization. Designed model saving/loading functionalities.</p>
                <p>Technologies: <span class="tag">Python</span> <span class="tag">PyTorch</span> <span class="tag">CNN</span> <span class="tag">MLP</span> <span class="tag">RNN</span> <span class="tag">Regression</span></p>
            </div>
            <div class="project">
                <span class="title">Summer Undergraduate Research Fellow, MetaTeddy: Interactive Reality Modeling Interface for 3D Freeform Design</span>
                <span class="date">Timeline: Jun. 2023 - Sept. 2023</span>
                <p class="role">Role: Research Fellow</p>
                <p class="description">Gathered literature for implementation principles. Developed a 2D interactive interface using Unity (Teddy system framework). Implemented cutting operations (CGAL) and enhanced model manipulation (TouchScript). Integrated XR technologies (Microsoft Hololens 2). Combined Meta Teddy with AR headsets via Unity and AR Foundation. Used Node.js with Socket.io for network connections between server and AR glasses.</p>
                <p>Technologies: <span class="tag">Unity</span> <span class="tag">CGAL</span> <span class="tag">Hololens 2</span> <span class="tag">AR</span> <span class="tag">Node.js</span> <span class="tag">XR</span></p>
            </div>
        </section>

        <section id="skills" class="section">
            <h2>Skills</h2>
            <h3>Programming Languages & Tools</h3>
            <p>
                <span class="tag">Python (Pandas, Numpy, Scikit-learn, TensorFlow, PyTorch, pytest)</span>
                <span class="tag">Java (JUnit, Spring Boot)</span>
                <span class="tag">SQL (MySQL, Snowflake)</span>
                <span class="tag">R</span> <span class="tag">Matlab</span> <span class="tag">Git</span>
            </p>
            <h3>Technical Domains</h3>
            <p>
                <span class="tag">Machine Learning</span> <span class="tag">Deep Learning</span>
                <span class="tag">Data Analysis & Visualization</span> <span class="tag">Software Engineering</span>
                <span class="tag">Computer Vision</span> <span class="tag">Multimodal Learning</span>
                <span class="tag">Full-Stack Development</span> <span class="tag">XR Development</span>
            </p>
            <h3>Languages</h3>
            <p>
                <span class="tag">Chinese (Native)</span>
                <span class="tag">English (Fluent)</span>
            </p>
        </section>

        <section id="cv" class="section">
            <h2>CV / Resume</h2>
            <p class="cv-link">
                <a href="Tianyi Wu CV 5.6.pdf" target="_blank">Download My CV (PDF) <span class="external-link-icon"></span></a>
            </p>
            <p style="font-size:0.9em; color:#777;">(Ensure the CV file "Tianyi Wu CV 5.6.pdf" is in the same directory as this HTML file, or update the link path.)</p>
        </section>

        <section id="contact" class="section">
            <h2>Contact</h2>
            <p>Email: <a href="mailto:18895396922@163.com">18895396922@163.com</a></p>
            <p>GitHub: <a href="https://github.com/EthanTianyiWu" target="_blank">github.com/EthanTianyiWu <span class="external-link-icon"></span></a></p>
            </section>
    </div>

    <footer>
        <p>&copy; 2025 Tianyi Wu. All rights reserved.</p>
        <p>Webpage content based on provided CV and design proposal.</p>
    </footer>

</body>
</html>
